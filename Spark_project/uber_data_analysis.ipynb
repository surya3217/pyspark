{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/26 12:07:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------+------+---------------+--------+--------------+\n",
      "|     Date|Time (Local)|Eyeballs|Zeroes|Completed Trips|Requests|Unique Drivers|\n",
      "+---------+------------+--------+------+---------------+--------+--------------+\n",
      "|10-Sep-12|           7|       5|     0|              2|       2|             9|\n",
      "|10-Sep-12|           8|       6|     0|              2|       2|            14|\n",
      "|10-Sep-12|           9|       8|     3|              0|       0|            14|\n",
      "|10-Sep-12|          10|       9|     2|              0|       1|            14|\n",
      "|10-Sep-12|          11|      11|     1|              4|       4|            11|\n",
      "|10-Sep-12|          12|      12|     0|              2|       2|            11|\n",
      "|10-Sep-12|          13|       9|     1|              0|       0|             9|\n",
      "|10-Sep-12|          14|      12|     1|              0|       0|             9|\n",
      "|10-Sep-12|          15|      11|     2|              1|       2|             7|\n",
      "|10-Sep-12|          16|      11|     2|              3|       4|             6|\n",
      "|10-Sep-12|          17|      12|     2|              3|       4|             4|\n",
      "|10-Sep-12|          18|      11|     1|              3|       4|             7|\n",
      "|10-Sep-12|          19|      13|     2|              2|       3|             7|\n",
      "|10-Sep-12|          20|      11|     1|              0|       0|             5|\n",
      "|10-Sep-12|          21|      11|     0|              1|       1|             3|\n",
      "|10-Sep-12|          22|      16|     3|              0|       2|             4|\n",
      "|10-Sep-12|          23|      21|     5|              3|       3|             4|\n",
      "|11-Sep-12|           0|       9|     3|              1|       1|             3|\n",
      "|11-Sep-12|           1|       3|     2|              0|       1|             3|\n",
      "|11-Sep-12|           2|       1|     1|              0|       0|             1|\n",
      "+---------+------------+--------+------+---------------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Uber_App\").getOrCreate()\n",
    "# df = spark.read.options(header=\"True\", inferSchema=\"True\").csv('/FileStore/tables/dataset-2.csv')\n",
    "# or\n",
    "df = spark.read.csv('/Users/suraj/Desktop/Documents/development_repo/pyspark/files/dataset.csv', header=\"True\", inferSchema=\"True\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time (Local): integer (nullable = true)\n",
      " |-- Eyeballs: integer (nullable = true)\n",
      " |-- Zeroes: integer (nullable = true)\n",
      " |-- Completed Trips: integer (nullable = true)\n",
      " |-- Requests: integer (nullable = true)\n",
      " |-- Unique Drivers: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Uber Data:\n",
    "1) Which date had the most completed trips during the two-week period?\n",
    "2) What was the highest number of completed trips within a 24-hour period?\n",
    "3) Which hour of the day had the most requests during the two-week period?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'22-Sep-12'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Which date had the most completed trips during the two-week period?\n",
    "\n",
    "completed_trips = df.groupBy(\"Date\").sum('Completed Trips')\n",
    "completed_trips.orderBy(col(\"sum(Completed Trips)\").desc() ).select('Date').first()['Date']\n",
    "\n",
    "# completed_trips.orderBy(col(\"sum(Completed Trips)\").desc() ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"window(Time (Local), 86400000000, 86400000000, 0)\" due to data type mismatch: Parameter 1 requires the (\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\" or \"STRUCT<start: TIMESTAMP, end: TIMESTAMP>\" or \"STRUCT<start: TIMESTAMP_NTZ, end: TIMESTAMP_NTZ>\") type, however \"Time (Local)\" has the type \"INT\".;\n'Aggregate [window(Time (Local)#18, 86400000000, 86400000000, 0)], [window(Time (Local)#18, 86400000000, 86400000000, 0) AS window#96, sum(Completed Trips#21) AS Total Completed Trips#105L]\n+- Relation [Date#17,Time (Local)#18,Eyeballs#19,Zeroes#20,Completed Trips#21,Requests#22,Unique Drivers#23] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     11\u001b[0m df_with_rank \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompleted Trips\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mover(trip_window))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# df_with_rank.show()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# completed_trips_by_window = df \\\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     .groupBy(\"Time (Local)\") \\\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     .agg(sum(\"Completed Trips\").alias(\"Total Completed Trips\")) \\\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#     .orderBy(\"Total Completed Trips\", ascending=False)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m completed_trips_by_window \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTime (Local)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m24 hours\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCompleted Trips\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTotal Completed Trips\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Completed Trips\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# completed_trips_by_window.show()\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# df.withColumn('time', rank().over(window)).show()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# .orderBy(\"Total Completed Trips\", ascending=False)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# display(completed_trips_by_window)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/sql/group.py:186\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m cast(Tuple[Column, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], exprs)\n\u001b[0;32m--> 186\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"window(Time (Local), 86400000000, 86400000000, 0)\" due to data type mismatch: Parameter 1 requires the (\"(TIMESTAMP OR TIMESTAMP WITHOUT TIME ZONE)\" or \"STRUCT<start: TIMESTAMP, end: TIMESTAMP>\" or \"STRUCT<start: TIMESTAMP_NTZ, end: TIMESTAMP_NTZ>\") type, however \"Time (Local)\" has the type \"INT\".;\n'Aggregate [window(Time (Local)#18, 86400000000, 86400000000, 0)], [window(Time (Local)#18, 86400000000, 86400000000, 0) AS window#96, sum(Completed Trips#21) AS Total Completed Trips#105L]\n+- Relation [Date#17,Time (Local)#18,Eyeballs#19,Zeroes#20,Completed Trips#21,Requests#22,Unique Drivers#23] csv\n"
     ]
    }
   ],
   "source": [
    "# 2) What was the highest number of completed trips within a 24-hour period?\n",
    "\n",
    "from pyspark.sql.functions import count, sum, rank\n",
    "\n",
    "# completed_trips_by_window = df.groupBy(window(\"Time (Local)\", \"24 hours\"))\n",
    "# completed_trips_by_window.agg(sum('Completed Trips')).show()\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "trip_window = Window.partitionBy(\"Time (Local)\").orderBy(\"Completed Trips\")\n",
    "df_with_rank = df.withColumn(\"sum\", sum(col('Completed Trips')).over(trip_window))\n",
    "# df_with_rank.show()\n",
    "\n",
    "# completed_trips_by_window = df \\\n",
    "#     .groupBy(\"Time (Local)\") \\\n",
    "#     .agg(sum(\"Completed Trips\").alias(\"Total Completed Trips\")) \\\n",
    "#     .orderBy(\"Total Completed Trips\", ascending=False)\n",
    "\n",
    "completed_trips_by_window = df \\\n",
    "    .groupBy(window(\"Time (Local)\", \"24 hours\")) \\\n",
    "    .agg(sum(\"Completed Trips\").alias(\"Total Completed Trips\")) \\\n",
    "    .orderBy(\"Total Completed Trips\", ascending=False)\n",
    "\n",
    "\n",
    "# completed_trips_by_window.show()\n",
    "\n",
    "# df.withColumn('time', rank().over(window)).show()\n",
    "# df = df.withColumn(\"Completed Trips\", df[\"Completed Trips\"].cast(IntegerType()))\n",
    "# df = df.filter(df[\"Completed Trips\"].isNotNull())\n",
    "# df.select('Completed Trips').show()\n",
    "\n",
    "# Group the data by 24-hour windows and sum the completed trips\n",
    "# completed_trips_by_window = df \\\n",
    "#     .groupBy(window(\"Time (Local)\", \"24 hours\")) \\\n",
    "#     .agg(count(\"Completed Trips\").alias(\"Total Completed Trips\")) \n",
    "    # .orderBy(\"Total Completed Trips\", ascending=False)\n",
    "# display(completed_trips_by_window)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour: 23\n"
     ]
    }
   ],
   "source": [
    "# 3) Which hour of the day had the most requests during the two-week period?\n",
    "\n",
    "most_requests = df.groupBy('Time (Local)').agg(sum(\"Requests\").alias(\"total_request\"))\n",
    "most_requests_hr = most_requests.orderBy('total_request', ascending=False).first()['Time (Local)']\n",
    "print('hour:',most_requests_hr)\n",
    "\n",
    "# most_requests.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496\n",
      "1429\n",
      "34.70958712386284\n"
     ]
    }
   ],
   "source": [
    "# 4) What percentages of all zeroes during the two-week period occurred on weekend (Friday at 5 pm to Sunday at 3 am)? \n",
    "\n",
    "from pyspark.sql.functions import dayofweek, hour, to_date\n",
    "\n",
    "df = df.withColumn(\"converted_date\", to_date(\"Date\", \"dd-MMM-yy\"))   # date is in string format so changed\n",
    "df = df.withColumn(\"week\", dayofweek(\"converted_date\"))\n",
    "\n",
    "weekend_zeros = df.filter( (col(\"Time (Local)\") >= 17) | (col(\"Time (Local)\") < 3) ).filter((col(\"week\") == 6) | (col(\"week\") == 7) | (col(\"week\") == 5))\n",
    "\n",
    "weekend_zeros_count = weekend_zeros.agg(sum(\"Zeroes\").alias(\"weekend_zeros\")) \\\n",
    "                    .collect()[0][\"weekend_zeros\"]\n",
    "\n",
    "print(weekend_zeros_count)\n",
    "# weekend_zeros.show()\n",
    "# df.printSchema()\n",
    "\n",
    "total_zeros_count = df.agg(sum('Zeroes').alias('total_zeros') ).collect()[0]['total_zeros']\n",
    "print(total_zeros_count)\n",
    "\n",
    "percentages_zeros = (weekend_zeros_count/total_zeros_count)*100\n",
    "print(percentages_zeros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5) What is the weighted average ratio of completed trips per driver during the two-week period? \n",
    "Tip: “Weighted average” means your answer should account for the total trip volume in each hour to determine the most accurate number in the whole period.\n",
    "\n",
    "6) In drafting a driver schedule in terms of 8 hours shifts, when are the busiest 8 consecutive hours over the two-week period in terms of unique requests? \n",
    "A new shift starts every 8 hours. Assume that a driver will work the same shift each day.\n",
    "\n",
    "7) True or False: Driver supply always increases when demand increases during the two-week period. Tip: Visualize the data to confirm your answer if needed.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|(sum(weighted_ratio) / sum(total_completed_trips))|\n",
      "+--------------------------------------------------+\n",
      "|                                0.8276707747535554|\n",
      "+--------------------------------------------------+\n",
      "\n",
      "0.8276707747535554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 5) What is the weighted average ratio of completed trips per driver during the two-week period? \n",
    "# Tip: “Weighted average” means your answer should account for the total trip volume in each hour to determine the most accurate number in the whole period.\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# a= (completed trip / unique driver, total no of completed trips for that hour, then sum result) // total no. of completed trips\n",
    "\n",
    "weighted_avg = df.withColumn(\"completed_per_driver\", df[\"Completed Trips\"] / df[\"Unique Drivers\"]) \\\n",
    "                 .groupBy(\"Date\", \"Time (Local)\").agg(avg(\"completed_per_driver\").alias(\"avg_completed_per_driver\"), sum(\"Completed Trips\").alias(\"total_completed_trips\") )\n",
    "# weighted_avg.show()\n",
    "\n",
    "weighted_ratio = weighted_avg.withColumn(\"weighted_ratio\", col(\"avg_completed_per_driver\") * col(\"total_completed_trips\") )\n",
    "# weighted_ratio.show()\n",
    "\n",
    "weighted_avg_ratio = weighted_ratio.agg(sum(\"weighted_ratio\") / sum(\"total_completed_trips\"))  # .collect()[0][0]\n",
    "weighted_avg_ratio.show()\n",
    "print(weighted_avg_ratio.collect()[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/26 16:30:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/26 16:30:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/26 16:30:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/26 16:30:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/26 16:30:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+---------------+\n",
      "|hour|unique_requests|consecutive_sum|\n",
      "+----+---------------+---------------+\n",
      "|  20|             12|             80|\n",
      "+----+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/26 16:30:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/26 16:30:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/26 16:30:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/12/26 16:30:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 6) In drafting a driver schedule in terms of 8 hours shifts, when are the busiest 8 consecutive hours over the two-week period in terms \n",
    "# of unique requests? \n",
    "# A new shift starts every 8 hours. Assume that a driver will work the same shift each day.\n",
    "\n",
    "from pyspark.sql.functions import col, hour, countDistinct\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "hourly_unique_requests = df.groupby(col(\"Time (Local)\").alias('hour')).agg(countDistinct('Requests').alias('unique_requests'))\n",
    "# hourly_unique_requests.show()\n",
    "\n",
    "# Slide a window of 8 hours to find the busiest 8 consecutive hours\n",
    "window = Window.orderBy(col(\"unique_requests\").desc()).rowsBetween(0, 7)\n",
    "\n",
    "busiest_8_consecutive_hours = (hourly_unique_requests\n",
    "  .select(\"*\", sum(\"unique_requests\").over(window).alias(\"consecutive_sum\"))\n",
    "  .orderBy(col(\"consecutive_sum\").desc())\n",
    "  .limit(1)\n",
    ")\n",
    "\n",
    "busiest_8_consecutive_hours.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) True or False: Driver supply always increases when demand increases during the two-week period. Tip: Visualize the data to confirm \n",
    "# your answer if needed.\n",
    "False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'dayofweek(2012-09-10)'>\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "a= '10-Sep-12'\n",
    "# b = datetime.date(a)\n",
    "\n",
    "print(dayofweek('2012-09-10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "8) In which 72-hour period is the ratio of Zeroes to Eyeballs the highest?\n",
    "\n",
    "9) If you could add 5 drivers to any single hour of every day during the two-week period, which hour should you add them to? \n",
    "Hint: Consider both rider eyeballs and driver supply when choosing.\n",
    "\n",
    "10) Looking at the data from all two weeks, which time might make the most sense to consider a true “end day” instead of midnight? \n",
    "(i.e when are supply and demand at both their natural minimums)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+-------------------+\n",
      "|period|zeroes|eyeballs|              ratio|\n",
      "+------+------+--------+-------------------+\n",
      "|  NULL|  1429|    6687|0.21369822042769554|\n",
      "+------+------+--------+-------------------+\n",
      "\n",
      "DataFrame[period: int, zeroes: bigint, eyeballs: bigint, ratio: double]\n"
     ]
    }
   ],
   "source": [
    "# 8) In which 72-hour period is the ratio of Zeroes to Eyeballs the highest?\n",
    "\n",
    "# Group the data by 72-hour periods and calculate the ratio of zeroes to eyeballs for each period\n",
    "period_ratios = (df\n",
    "  .groupBy(((col(\"Date\").cast(\"timestamp\").cast(\"long\") / (72*3600)).cast(\"int\")).alias(\"period\"))\n",
    "  .agg(sum(\"Zeroes\").alias(\"zeroes\"), sum(\"Eyeballs\").alias(\"eyeballs\"))\n",
    "  .withColumn(\"ratio\", col(\"zeroes\") / col(\"eyeballs\"))\n",
    ")\n",
    "period_ratios.show()\n",
    "\n",
    "# Find the period with the highest ratio\n",
    "highest_ratio_period = period_ratios.orderBy(col(\"ratio\").desc()).limit(1)\n",
    "print(highest_ratio_period)\n",
    "\n",
    "# trip_window = Window.orderBy(\"Date\").rowsBetween(0,3)\n",
    "# window = Window.orderBy(col(\"unique_requests\").desc()).rowsBetween(0, 7)\n",
    "# period_ratios = df.select(\"*\", sum(\"Zeroes\").over(trip_window), sum(\"Eyeballs\").over(trip_window) )\n",
    "\n",
    "# period_ratios = df.groupby(trip_window).agg(sum(\"Zeroes\"))\n",
    "# period_ratios.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(hour=2, request_driver_ratio=1.6129032258064515)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9) If you could add 5 drivers to any single hour of every day during the two-week period, which hour should you add them to? \n",
    "# Hint: Consider both rider eyeballs and driver supply when choosing.\n",
    "\n",
    "ratio = df.groupby(col(\"Time (Local)\").alias(\"hour\") ).agg((sum(\"Requests\")/sum(\"Unique Drivers\")).alias(\"request_driver_ratio\") )\n",
    "# ratio.show()\n",
    "\n",
    "ratio.orderBy(col(\"request_driver_ratio\").desc() ).collect()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------------+\n",
      "|hour|avg_completed_trips|avg_unique_drivers|\n",
      "+----+-------------------+------------------+\n",
      "|  12| 3.2857142857142856| 9.428571428571429|\n",
      "|  22|  9.571428571428571|10.285714285714286|\n",
      "|   1|  5.071428571428571| 6.714285714285714|\n",
      "|  13|  3.142857142857143| 8.714285714285714|\n",
      "|  16|  4.857142857142857|10.285714285714286|\n",
      "|   6| 1.3571428571428572| 2.642857142857143|\n",
      "|   3|                1.5| 2.857142857142857|\n",
      "|  20|  5.428571428571429|11.642857142857142|\n",
      "|   5| 0.2857142857142857|0.7857142857142857|\n",
      "|  19|  8.571428571428571|12.857142857142858|\n",
      "|  15|  3.357142857142857| 9.928571428571429|\n",
      "|   9| 1.4285714285714286| 7.857142857142857|\n",
      "|  17|  5.571428571428571|11.785714285714286|\n",
      "|   4|0.14285714285714285|0.6428571428571429|\n",
      "|   8| 1.7142857142857142| 6.785714285714286|\n",
      "|  23|                9.0|               8.5|\n",
      "|   7| 1.3571428571428572| 4.285714285714286|\n",
      "|  10| 1.2857142857142858| 9.214285714285714|\n",
      "|  21|  6.285714285714286|11.071428571428571|\n",
      "|  11| 2.5714285714285716|               9.5|\n",
      "+----+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+-------------------+------------------+\n",
      "|hour|avg_completed_trips|avg_unique_drivers|\n",
      "+----+-------------------+------------------+\n",
      "|   4|0.14285714285714285|0.6428571428571429|\n",
      "+----+-------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10) Looking at the data from all two weeks, which time might make the most sense to consider a true “end day” instead of midnight? \n",
    "# (i.e when are supply and demand at both their natural minimums)\n",
    "from pyspark.sql.functions import col, mean\n",
    "\n",
    "avg_trips_and_drivers = df.groupby(col(\"Time (Local)\").alias(\"hour\") ).agg(avg(\"Completed Trips\").alias(\"avg_completed_trips\"), avg(\"Unique Drivers\").alias(\"avg_unique_drivers\"))\n",
    "avg_trips_and_drivers.show()\n",
    "\n",
    "avg_trips_and_drivers.orderBy('avg_completed_trips', 'avg_unique_drivers').show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
